{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 任务描述\n",
    "利用飞桨框架搭建一个Vision Transformer模型，对包含不同花朵的图像进行分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  def convert_to_list(value, n, name, dtype=np.int):\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/__init__.py:107: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import MutableMapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable, Mapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sized\n",
      "2023-01-04 21:05:23,441 - INFO - font search path ['/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf', '/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/afm', '/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/pdfcorefonts']\n"
     ]
    }
   ],
   "source": [
    "import os\r\n",
    "import zipfile\r\n",
    "import random\r\n",
    "import paddle\r\n",
    "from paddle import fluid\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import PIL.Image as Image\r\n",
    "from paddle.io import Dataset \r\n",
    "import numpy as np \r\n",
    "import sys\r\n",
    "import paddle.nn as nn\r\n",
    "from multiprocessing import cpu_count  \r\n",
    "from paddle.nn import MaxPool2D,Conv2D,BatchNorm2D\r\n",
    "from paddle.nn import Linear \r\n",
    "import random\r\n",
    "from paddle.nn.initializer import TruncatedNormal, Constant \r\n",
    "from paddle.nn import TransformerEncoderLayer, TransformerEncoder\r\n",
    "from paddle.regularizer import L2Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.配置模型参数\n",
    "配置模型参数，并且设置随机数种子，确保相同参数配置时，结果可复现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cp work/checkpoints/save_dir_final.pdparams ../log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\r\n",
    "参数配置\r\n",
    "'''\r\n",
    "train_parameters = {\r\n",
    "    \"input_size\": [3, 120, 120],                             #输入图片的shape\r\n",
    "    \"class_dim\": 3,                                          #分类数\r\n",
    "    \"src_path\": \"/home/aistudio/data/data30788/54_data.zip\",                       #原始数据集路径\r\n",
    "    \"target_path\": \"/home/aistudio/data/\",                    #要解压的路径\r\n",
    "    \"train_list_path\": \"/home/aistudio/data/train.txt\",      #train.txt路径\r\n",
    "    \"eval_list_path\": \"/home/aistudio/data/eval.txt\",        #eval.txt路径\r\n",
    "    \"num_epochs\": 4,                                        #训练轮数\r\n",
    "    \"train_batch_size\": 16,                                   #训练时每个批次的大小\r\n",
    "    \"learning_strategy\": {                                   #优化函数相关的配置\r\n",
    "        \"lr\": 1.0e-5                                        #超参数学习率\r\n",
    "    }, \r\n",
    "    'skip_steps': 50,                                        #每N个批次打印一次结果\r\n",
    "    'save_steps': 500,                                       #每N个批次保存一次模型参数\r\n",
    "    \"checkpoints\": \"/home/aistudio/work/checkpoints\"         #保存的路径\r\n",
    "}\r\n",
    "\r\n",
    "def seed_paddle(seed=1024):\r\n",
    "    seed = int(seed)\r\n",
    "    random.seed(seed)\r\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\r\n",
    "    np.random.seed(seed)\r\n",
    "    paddle.seed(seed)\r\n",
    "\r\n",
    "seed_paddle(seed=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. 数据集准备\n",
    "(1)解压原始数据集\n",
    "\n",
    "(2)按照比例划分训练集与验证集\n",
    "\n",
    "(3)乱序，生成数据列表\n",
    "\n",
    "(4)定义数据读取器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "# 解压原始数据集\r\n",
    "def unzip_data(src_path,target_path):\r\n",
    "    if(not os.path.isdir(target_path + \"54_data\")):     \r\n",
    "        z = zipfile.ZipFile(src_path, 'r')\r\n",
    "        z.extractall(path=target_path)\r\n",
    "        z.close()\r\n",
    "\r\n",
    "def get_data_list(target_path, train_list_path, eval_list_path):\r\n",
    "    '''\r\n",
    "    生成数据列表\r\n",
    "    '''\r\n",
    "    # train_txt = target_path + \"54_data/train.csv\"\r\n",
    "    # eval_txt = \"/home/aistudio/data/data30788/test.csv\"\r\n",
    "    # #获取所有类别保存的文件夹名称\r\n",
    "    # path_train = \"/home/aistudio/data/54_data/train\"\r\n",
    "    # path_test =  \"/home/aistudio/data/54_data/test\"\r\n",
    "    # #存放类别数目\r\n",
    "    # class_dim = 0\r\n",
    "    # #存储要写进eval.txt和train.txt中的内容\r\n",
    "    # trainer_list=[]\r\n",
    "    # eval_list=[]\r\n",
    "    # #读取每个类别\r\n",
    "    # DF = pd.read_csv(train_txt)\r\n",
    "    # for i in range(len(DF.index)):\r\n",
    "    #     dim_1 = max(DF.iloc[:]['label'])+1\r\n",
    "    #     img_path = DF.iloc[i][0]\r\n",
    "    #     class_label = DF.iloc[i][1]\r\n",
    "    #     name_path = path_train + '/' + img_path\r\n",
    "    #     trainer_list.append(name_path + \"\\t%d\" % class_label + \"\\n\")  \r\n",
    "    # DF2 = pd.read_csv(eval_txt,header=None)\r\n",
    "    # for i in range(len(DF2.index)):\r\n",
    "    #     dim_2 = max(DF2.iloc[:][1])+1\r\n",
    "    #     img_path = DF2.iloc[i][0]\r\n",
    "    #     class_label = DF2.iloc[i][1]\r\n",
    "    #     name_path = path_test + '/' + img_path\r\n",
    "    #     eval_list.append(name_path + \"\\t%d\" % class_label + \"\\n\")  \r\n",
    "\r\n",
    "    # class_dim = max(dim_1,dim_2)\r\n",
    "    #存放所有类别的信息\r\n",
    "    class_detail = []\r\n",
    "    #获取所有类别保存的文件夹名称\r\n",
    "    data_list_path=\"/home/aistudio/data/54_data/\"\r\n",
    "    class_dirs = os.listdir(data_list_path)  \r\n",
    "    #总的图像数量\r\n",
    "    all_class_images = 0\r\n",
    "    #存放类别标签\r\n",
    "    class_label=0\r\n",
    "    #存放类别数目\r\n",
    "    class_dim = 0\r\n",
    "    #存储要写进eval.txt和train.txt中的内容\r\n",
    "    trainer_list=[]\r\n",
    "    eval_list=[]\r\n",
    "    #读取每个类别\r\n",
    "    for class_dir in class_dirs:\r\n",
    "        if class_dir != \".DS_Store\" and class_dir != \"train.csv\":\r\n",
    "            class_dim += 1\r\n",
    "            #每个类别的信息\r\n",
    "            class_detail_list = {}\r\n",
    "            eval_sum = 0\r\n",
    "            trainer_sum = 0\r\n",
    "            #统计每个类别有多少张图片\r\n",
    "            class_sum = 0\r\n",
    "            #获取类别路径 \r\n",
    "            path = data_list_path  + class_dir\r\n",
    "            # 获取所有图片\r\n",
    "            img_paths = os.listdir(path)\r\n",
    "            for img_path in img_paths:                                  # 遍历文件夹下的每个图片\r\n",
    "                if img_path.split(\".\")[-1] == \"jpg\":\r\n",
    "                    name_path = path + '/' + img_path                       # 每张图片的路径\r\n",
    "                    if class_sum % 8 == 0:                                  # 每8张图片取一个做验证数据\r\n",
    "                        eval_sum += 1                                       # test_sum为测试数据的数目\r\n",
    "                        eval_list.append(name_path + \"\\t%d\" % class_label + \"\\n\")\r\n",
    "                    else:\r\n",
    "                        trainer_sum += 1 \r\n",
    "                        trainer_list.append(name_path + \"\\t%d\" % class_label + \"\\n\")#trainer_sum测试数据的数目\r\n",
    "                    class_sum += 1                                          #每类图片的数目\r\n",
    "                    all_class_images += 1                                   #所有类图片的数目\r\n",
    "                else:\r\n",
    "                    continue\r\n",
    "            # 说明的json文件的class_detail数据\r\n",
    "            class_detail_list['class_name'] = class_dir             #类别名称\r\n",
    "            class_detail_list['class_label'] = class_label          #类别标签\r\n",
    "            class_detail_list['class_eval_images'] = eval_sum       #该类数据的测试集数目\r\n",
    "            class_detail_list['class_trainer_images'] = trainer_sum #该类数据的训练集数目\r\n",
    "            class_detail.append(class_detail_list)  \r\n",
    "            #初始化标签列表\r\n",
    "            #train_parameters['label_dict'][str(class_label)] = class_dir\r\n",
    "            class_label += 1 \r\n",
    "\r\n",
    "    #初始化分类数\r\n",
    "    train_parameters['class_dim'] = class_dim\r\n",
    "  \r\n",
    "    #乱序  \r\n",
    "    random.shuffle(eval_list)\r\n",
    "    with open(eval_list_path, 'a') as f:\r\n",
    "        for eval_image in eval_list:\r\n",
    "            f.write(eval_image) \r\n",
    "            \r\n",
    "    random.shuffle(trainer_list)\r\n",
    "    with open(train_list_path, 'a') as f2:\r\n",
    "        for train_image in trainer_list:\r\n",
    "            f2.write(train_image) \r\n",
    "    print ('生成数据列表完成！')\r\n",
    "\r\n",
    "class dataset(Dataset):\r\n",
    "    def __init__(self, data_path, mode='train'):\r\n",
    "        \"\"\"\r\n",
    "        数据读取器\r\n",
    "        :param data_path: 数据集所在路径\r\n",
    "        :param mode: train or eval\r\n",
    "        \"\"\"\r\n",
    "        super().__init__()\r\n",
    "        self.data_path = data_path\r\n",
    "        self.img_paths = []\r\n",
    "        self.labels = []\r\n",
    "\r\n",
    "        if mode == 'train':\r\n",
    "            with open(os.path.join(self.data_path, \"train.txt\"), \"r\", encoding=\"utf-8\") as f:\r\n",
    "                self.info = f.readlines()\r\n",
    "            for img_info in self.info:\r\n",
    "                img_path, label = img_info.strip().split('\\t')\r\n",
    "                self.img_paths.append(img_path)\r\n",
    "                self.labels.append(int(label))\r\n",
    "\r\n",
    "        else:\r\n",
    "            with open(os.path.join(self.data_path, \"eval.txt\"), \"r\", encoding=\"utf-8\") as f:\r\n",
    "                self.info = f.readlines()\r\n",
    "            for img_info in self.info:\r\n",
    "                img_path, label = img_info.strip().split('\\t')\r\n",
    "                self.img_paths.append(img_path)\r\n",
    "                self.labels.append(int(label))\r\n",
    "\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        \"\"\"\r\n",
    "        获取一组数据\r\n",
    "        :param index: 文件索引号\r\n",
    "        :return:\r\n",
    "        \"\"\"\r\n",
    "         # 第一步打开图像文件并获取label值\r\n",
    "        img_path = self.img_paths[index]\r\n",
    "        img = Image.open(img_path)\r\n",
    "        if img.mode != 'RGB':\r\n",
    "            img = img.convert('RGB') \r\n",
    "        img = img.resize((120, 120), Image.BILINEAR)\r\n",
    "        img = np.array(img).astype('float32')\r\n",
    "        img = img.transpose((2, 0, 1)) / 255\r\n",
    "        label = self.labels[index]\r\n",
    "        label = np.array([label], dtype=\"int64\")\r\n",
    "        return img, label\r\n",
    "\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.img_paths)\r\n",
    "\r\n",
    "\r\n",
    "#参数初始化\r\n",
    "src_path=train_parameters['src_path']\r\n",
    "target_path=train_parameters['target_path']\r\n",
    "train_list_path=train_parameters['train_list_path']\r\n",
    "eval_list_path=train_parameters['eval_list_path']\r\n",
    "\r\n",
    "#解压原始数据到指定路径\r\n",
    "unzip_data(src_path,target_path)\r\n",
    "\r\n",
    "#每次生成数据列表前，首先清空train.txt和eval.txt\r\n",
    "with open(train_list_path, 'w') as f: \r\n",
    "    f.seek(0)\r\n",
    "    f.truncate() \r\n",
    "with open(eval_list_path, 'w') as f: \r\n",
    "    f.seek(0)\r\n",
    "    f.truncate()     \r\n",
    "\r\n",
    "#生成数据列表   \r\n",
    "get_data_list(target_path,train_list_path,eval_list_path)\r\n",
    "print(\" train_parameters['class_dim']:  {}\".format(train_parameters['class_dim']))\r\n",
    "\r\n",
    "#训练数据加载\r\n",
    "train_dataset = dataset('/home/aistudio/data',mode='train')\r\n",
    "train_loader = paddle.io.DataLoader(train_dataset, batch_size=16, shuffle=True)\r\n",
    "#测试数据加载\r\n",
    "eval_dataset = dataset('/home/aistudio/data',mode='eval')\r\n",
    "eval_loader = paddle.io.DataLoader(eval_dataset, batch_size = 8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义ViT\r\n",
    "trunc_normal_ = TruncatedNormal(std=.02)\r\n",
    "zeros_ = Constant(value=0.)\r\n",
    "ones_ = Constant(value=1.)\r\n",
    "dim = 128\r\n",
    "heads = 8\r\n",
    "patch_size = 4\r\n",
    "num_layers = 1\r\n",
    "num_patch = int((120/patch_size) * (120/patch_size))\r\n",
    "\r\n",
    "# x[int] -> tuple(x, x)\r\n",
    "def to_2tuple(x):\r\n",
    "    return tuple([x] * 2)\r\n",
    "\r\n",
    "# 独立层，即什么操作都没有的网络层\r\n",
    "class Identity(nn.Layer):\r\n",
    "    def __init__(self):\r\n",
    "        super(Identity, self).__init__()\r\n",
    "\r\n",
    "    def forward(self, input):\r\n",
    "        return input\r\n",
    "\r\n",
    "class PatchEmbed(nn.Layer):\r\n",
    "    def __init__(self, img_size=120, patch_size=patch_size, in_chans=3, embed_dim=dim):\r\n",
    "        super().__init__()\r\n",
    "        img_size = to_2tuple(img_size)\r\n",
    "        patch_size = to_2tuple(patch_size)\r\n",
    "        num_patches = (img_size[1] // patch_size[1]) * \\\r\n",
    "            (img_size[0] // patch_size[0])\r\n",
    "        self.img_size = img_size\r\n",
    "        self.patch_size = patch_size\r\n",
    "        self.num_patches = num_patches\r\n",
    "\r\n",
    "        self.proj = nn.Conv2D(in_chans, embed_dim,\r\n",
    "                              kernel_size=patch_size, stride=patch_size)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        B, C, H, W = x.shape\r\n",
    "        # 分块线性变换 + 向量展平 + 维度转置\r\n",
    "        x = self.proj(x).flatten(2).transpose((0, 2, 1))\r\n",
    "        return x\r\n",
    "\r\n",
    "class Attention(nn.Layer):\r\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\r\n",
    "        super().__init__()\r\n",
    "        self.num_heads = num_heads\r\n",
    "        head_dim = dim // num_heads\r\n",
    "        self.scale = qk_scale or head_dim ** -0.5\r\n",
    "\r\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias_attr=qkv_bias)\r\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\r\n",
    "        self.proj = nn.Linear(dim, dim)\r\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        B, N, C = x.shape\r\n",
    "\r\n",
    "        # 线性变换\r\n",
    "        qkv = self.qkv(x).reshape((B, N, 3, self.num_heads, C //\r\n",
    "                                   self.num_heads)).transpose((2, 0, 3, 1, 4))\r\n",
    "        \r\n",
    "        # 分割 query key value\r\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\r\n",
    "        \r\n",
    "        # Matmul + Scale\r\n",
    "        attn = (q.matmul(k.transpose((0, 1, 3, 2)))) * self.scale\r\n",
    "\r\n",
    "        # SoftMax\r\n",
    "        attn = nn.functional.softmax(attn, axis=-1)\r\n",
    "        \r\n",
    "        # Attention Dropout\r\n",
    "        attn = self.attn_drop(attn)\r\n",
    "        \r\n",
    "        # Matmul\r\n",
    "        x = (attn.matmul(v)).transpose((0, 2, 1, 3)).reshape((B, N, C))\r\n",
    "\r\n",
    "        # 线性变换\r\n",
    "        x = self.proj(x)\r\n",
    "\r\n",
    "        # Linear Dropout\r\n",
    "        x = self.proj_drop(x)\r\n",
    "        return x\r\n",
    "\r\n",
    "class Mlp(nn.Layer):\r\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\r\n",
    "        super().__init__()\r\n",
    "        out_features = out_features or in_features\r\n",
    "        hidden_features = hidden_features or in_features\r\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\r\n",
    "        self.act = act_layer()\r\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\r\n",
    "        self.drop = nn.Dropout(drop)\r\n",
    "\r\n",
    "    def forward(self, x): \r\n",
    "        x = self.fc1(x) \r\n",
    "        x = self.act(x) \r\n",
    "        x = self.drop(x) \r\n",
    "        x = self.fc2(x) \r\n",
    "        x = self.drop(x)\r\n",
    "        return x\r\n",
    "\r\n",
    "def drop_path(x, drop_prob=0., training=False):\r\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\r\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\r\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ...\r\n",
    "    \"\"\"\r\n",
    "    if drop_prob == 0. or not training:\r\n",
    "        return x\r\n",
    "    keep_prob = paddle.to_tensor(1 - drop_prob)\r\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\r\n",
    "    random_tensor = keep_prob + paddle.rand(shape, dtype=x.dtype)\r\n",
    "    random_tensor = paddle.floor(random_tensor) # binarize\r\n",
    "    output = x.divide(keep_prob) * random_tensor\r\n",
    "    return output\r\n",
    "\r\n",
    "\r\n",
    "class DropPath(nn.Layer):\r\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, drop_prob=None):\r\n",
    "        super(DropPath, self).__init__()\r\n",
    "        self.drop_prob = drop_prob\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        return drop_path(x, self.drop_prob, self.training)\r\n",
    "\r\n",
    "\r\n",
    "# Block类实现Transformer encoder的一个层\r\n",
    "class Block(nn.Layer):\r\n",
    "    # 定义Transformer层\r\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\r\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer='nn.LayerNorm', epsilon=1e-5):\r\n",
    "        super().__init__()\r\n",
    "        # 此处添加代码\r\n",
    "        self.norm1 = eval(norm_layer)(dim, epsilon=epsilon)\r\n",
    "        self.attn = Attention(\r\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\r\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else Identity()\r\n",
    "        self.norm2 = eval(norm_layer)(dim, epsilon=epsilon)\r\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\r\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        # 此处添加代码\r\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\r\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\r\n",
    "        return x\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "class VisionTransformer(nn.Layer):\r\n",
    "    def __init__(self, img_size=120, patch_size=patch_size, in_chans=3, class_dim=train_parameters['class_dim'], embed_dim=dim, depth=num_layers,\r\n",
    "                 num_heads=heads, mlp_ratio=4, qkv_bias=False, qk_scale=None, drop_rate=0.2, attn_drop_rate=0.2,\r\n",
    "                 drop_path_rate=0., norm_layer='nn.LayerNorm', epsilon=1e-5, **args):\r\n",
    "        super().__init__()\r\n",
    "        self.class_dim = class_dim\r\n",
    "\r\n",
    "        self.num_features = self.embed_dim = embed_dim\r\n",
    "\r\n",
    "        self.patch_embed = PatchEmbed(\r\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\r\n",
    "        num_patches = self.patch_embed.num_patches\r\n",
    "\r\n",
    "        self.pos_embed = self.create_parameter(\r\n",
    "            shape=(1, num_patches + 1, embed_dim), default_initializer=zeros_)\r\n",
    "        self.add_parameter(\"pos_embed\", self.pos_embed)\r\n",
    "        self.cls_token = self.create_parameter(\r\n",
    "            shape=(1, 1, embed_dim), default_initializer=zeros_)\r\n",
    "        self.add_parameter(\"cls_token\", self.cls_token)\r\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\r\n",
    "\r\n",
    "        dpr = [x for x in paddle.linspace(0, drop_path_rate, depth)]\r\n",
    "\r\n",
    "        self.blocks = nn.LayerList([\r\n",
    "            Block(\r\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\r\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, epsilon=epsilon)\r\n",
    "            for i in range(depth)])\r\n",
    "\r\n",
    "        self.norm = eval(norm_layer)(embed_dim, epsilon=epsilon)\r\n",
    "\r\n",
    "        # Classifier head\r\n",
    "        self.head = nn.Linear(\r\n",
    "            embed_dim, class_dim) if class_dim > 0 else Identity()\r\n",
    "\r\n",
    "        trunc_normal_(self.pos_embed)\r\n",
    "        trunc_normal_(self.cls_token)\r\n",
    "        self.apply(self._init_weights)\r\n",
    "    \r\n",
    "    # 参数初始化\r\n",
    "    def _init_weights(self, m):\r\n",
    "        if isinstance(m, nn.Linear):\r\n",
    "            trunc_normal_(m.weight)\r\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\r\n",
    "                zeros_(m.bias)\r\n",
    "        elif isinstance(m, nn.LayerNorm):\r\n",
    "            zeros_(m.bias)\r\n",
    "            ones_(m.weight)\r\n",
    "    \r\n",
    "    # 获取图像特征\r\n",
    "    def forward_features(self, x):\r\n",
    "        B = x.shape[0] \r\n",
    "        # Image Patch Embedding\r\n",
    "        x = self.patch_embed(x) \r\n",
    "        # 分类 tokens\r\n",
    "        cls_tokens = self.cls_token.expand((B, -1, -1)) \r\n",
    "        # 拼接 Embedding 和 分类 tokens\r\n",
    "        x = paddle.concat((cls_tokens, x), axis=1) \r\n",
    "        # 加入位置嵌入 Position Embedding\r\n",
    "        x = x + self.pos_embed \r\n",
    "        # Embedding Dropout\r\n",
    "        x = self.pos_drop(x)\r\n",
    "        # Transformer Encoder\r\n",
    "        # 由多个基础模块组成\r\n",
    "        for blk in self.blocks:\r\n",
    "            x = blk(x) \r\n",
    "        # Norm\r\n",
    "        x = self.norm(x) \r\n",
    "        # 提取分类 tokens 的输出\r\n",
    "        return x[:, 0]\r\n",
    "    \r\n",
    "    def forward(self, x):\r\n",
    "        x = paddle.reshape(x, shape=[-1, 3,120,120])\r\n",
    "        # 获取图像特征\r\n",
    "        x = self.forward_features(x) \r\n",
    "        # 图像分类 \r\n",
    "        x = self.head(x) \r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5. 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def draw_process(title,color,iters,data,label):\r\n",
    "    plt.title(title, fontsize=24)\r\n",
    "    plt.xlabel(\"iter\", fontsize=20)\r\n",
    "    plt.ylabel(label, fontsize=20)\r\n",
    "    plt.plot(iters, data,color=color,label=label) \r\n",
    "    plt.legend()\r\n",
    "    plt.grid()\r\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = VisionTransformer()\r\n",
    "model.train()\r\n",
    "cross_entropy = paddle.nn.CrossEntropyLoss()\r\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=train_parameters['learning_strategy']['lr'],\r\n",
    "                                  parameters=model.parameters()) \r\n",
    "                                  \r\n",
    "steps = 0\r\n",
    "Iters, total_loss, total_acc = [], [], []\r\n",
    "\r\n",
    "for epo in range(train_parameters['num_epochs']):\r\n",
    "    for _, data in enumerate(train_loader()):\r\n",
    "        steps += 1\r\n",
    "        x_data = data[0]\r\n",
    "        y_data = data[1]\r\n",
    "        predicts = model(x_data)\r\n",
    "        loss = cross_entropy(predicts, y_data)\r\n",
    "        acc = paddle.metric.accuracy(predicts, y_data)\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        optimizer.clear_grad()\r\n",
    "        if steps % train_parameters[\"skip_steps\"] == 0:\r\n",
    "            Iters.append(steps)\r\n",
    "            total_loss.append(loss.numpy()[0])\r\n",
    "            total_acc.append(acc.numpy()[0])\r\n",
    "            #打印中间过程\r\n",
    "            print('epo: {}, step: {}, loss is: {}, acc is: {}'\\\r\n",
    "                  .format(epo, steps, loss.numpy(), acc.numpy()))\r\n",
    "        #保存模型参数\r\n",
    "        if steps % train_parameters[\"save_steps\"] == 0:\r\n",
    "            save_path = train_parameters[\"checkpoints\"]+\"/\"+\"save_dir_\" + str(steps) + '.pdparams'\r\n",
    "            print('save model to: ' + save_path)\r\n",
    "            paddle.save(model.state_dict(),save_path)\r\n",
    "paddle.save(model.state_dict(),train_parameters[\"checkpoints\"]+\"/\"+\"save_dir_final.pdparams\")\r\n",
    "draw_process(\"trainning loss\",\"red\",Iters,total_loss,\"trainning loss\")\r\n",
    "draw_process(\"trainning acc\",\"green\",Iters,total_acc,\"trainning acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 6. 评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model__state_dict = paddle.load(\"work/checkpoints/save_dir_1500.pdparams\")\r\n",
    "model_eval = VisionTransformer()\r\n",
    "model_eval.set_state_dict(model__state_dict) \r\n",
    "model_eval.eval()\r\n",
    "accs = []\r\n",
    "for _, data in enumerate(eval_loader()):\r\n",
    "    x_data = data[0]\r\n",
    "    y_data = data[1]\r\n",
    "    predicts = model_eval(x_data)\r\n",
    "    acc = paddle.metric.accuracy(predicts, y_data)\r\n",
    "    accs.append(acc.numpy()[0])\r\n",
    "print('模型在验证集上的准确率为：{}'.format(np.mean(accs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\r\n",
    "模型评估\r\n",
    "'''\r\n",
    "# 将所有的训练模型在验证集上进行测试\r\n",
    "filePath = 'work/checkpoints/'\r\n",
    "best_acc = {}\r\n",
    "for params in os.listdir(filePath):\r\n",
    "    model__state_dict = paddle.load(filePath+params)\r\n",
    "    model_eval = VisionTransformer()\r\n",
    "    model_eval.set_state_dict(model__state_dict) \r\n",
    "    model_eval.eval()\r\n",
    "    accs = []\r\n",
    "    for _, data in enumerate(eval_loader()):\r\n",
    "        x_data = data[0]\r\n",
    "        y_data = data[1]\r\n",
    "        predicts = model_eval(x_data)\r\n",
    "        acc = paddle.metric.accuracy(predicts, y_data)\r\n",
    "        accs.append(acc.numpy()[0])\r\n",
    "    best_acc.update({params:np.mean(accs)})#将模型和对应的acc保存到best_acc中\r\n",
    "    print(np.mean(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_acc = sorted(best_acc.items(),  key=lambda d: d[1], reverse=False) #将acc由小到大排序\r\n",
    "for key,value in new_acc:\r\n",
    "    print('【{}】模型在验证集上的准确率为：{}'.format(key,value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
